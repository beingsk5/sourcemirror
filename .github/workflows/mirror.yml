name: Public SourceForge Mirror

on:
  workflow_dispatch:
    inputs:
      job_id:
        required: true
        type: string
      urls:
        required: false
        type: string
      links:
        required: false
        type: string
      notes:
        required: false
        type: string
      retry_files:
        required: false
        type: string
      compression:
        required: false
        type: string

permissions:
  contents: write

jobs:
  mirror:
    runs-on: ubuntu-latest

    steps:

      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      # --------------------------------------------------
      # prepare progress branch (DO NOT CHANGE PATHING)
      # --------------------------------------------------
      - name: Prepare progress branch
        run: |
          git config user.name "progress-bot"
          git config user.email "progress@bot"

          git fetch origin

          if git show-ref --verify --quiet refs/remotes/origin/progress; then
            git checkout progress
            git pull origin progress
          else
            git checkout --orphan progress
            git rm -rf .
            mkdir -p progress
            git add progress
            git commit -m "init progress branch"
            git push origin progress
          fi

      # --------------------------------------------------
      # record job start time
      # --------------------------------------------------
      - name: Record job start time
        run: |
          mkdir -p work
          date -u +%s > work/start_time.txt

      # --------------------------------------------------
      # build working file list (UI compatible)
      # --------------------------------------------------
      - name: Build job file list
        run: |
          mkdir -p work

          python3 << 'EOF'
          import json, os

          links = os.environ.get("LINKS")
          urls  = os.environ.get("URLS")
          retry = os.environ.get("RETRY")
          compression_raw = os.environ.get("COMPRESSION")

          compression = None
          if compression_raw:
              try:
                  compression = json.loads(compression_raw)
              except:
                  compression = None

          def archive_ext(t):
              return {
                  "zip": "zip",
                  "7z": "7z",
                  "tar.gz": "tar.gz",
                  "tar.bz2": "tar.bz2",
                  "tar.xz": "tar.xz",
                  "gz": "gz",
                  "bz2": "bz2",
                  "xz": "xz"
              }.get(t, "zip")

          files = []

          # retry mode (ONLY via /retry API)
          if retry:
              try:
                  arr = json.loads(retry)
              except:
                  arr = []

              for f in arr:
                  files.append({
                      "url": "",
                      "original": f,
                      "final": f,
                      "folder": "",
                      "notes": "",
                      "status": "retry"
                  })

          elif links:
              try:
                  arr = json.loads(links)
              except:
                  arr = []

              for x in arr:
                  url = x.get("url","")
                  name = url.split("/")[-1] or "file"

                  base   = x.get("rename_base") or ""
                  folder = x.get("folder") or ""
                  notes  = x.get("notes") or ""

                  if not base:
                      base = name.rsplit(".",1)[0]

                  if compression and compression.get("enabled"):
                      final = base + "." + archive_ext(compression.get("type","zip"))
                  else:
                      r = x.get("rename")
                      final = r if r else name

                  files.append({
                      "url": url,
                      "original": name,
                      "final": final,
                      "folder": folder,
                      "notes": notes,
                      "status": "pending"
                  })

          elif urls:
              for u in urls.splitlines():
                  u = u.strip()
                  if not u:
                      continue

                  name = u.split("/")[-1] or "file"

                  files.append({
                      "url": u,
                      "original": name,
                      "final": name,
                      "folder": "",
                      "notes": "",
                      "status": "pending"
                  })

          with open("work/files.json","w") as f:
              json.dump({
                  "compression": compression,
                  "files": files
              }, f, indent=2)

          EOF
        env:
          LINKS: ${{ inputs.links }}
          URLS:  ${{ inputs.urls }}
          RETRY: ${{ inputs.retry_files }}
          COMPRESSION: ${{ inputs.compression }}

      # -----------------------------
      # validating
      # -----------------------------
      - name: Stage – validating
        run: |
          mkdir -p progress
          echo "validating" > progress/${{ inputs.job_id }}.txt
          git add progress/${{ inputs.job_id }}.txt
          git commit -m "stage validating ${{ inputs.job_id }}" || true
          git push origin progress

      # -----------------------------
      # downloading (FAST + fallback + multi-source)
      # -----------------------------
      - name: Stage – downloading
        run: |
          echo "downloading" > progress/${{ inputs.job_id }}.txt
          git add progress/${{ inputs.job_id }}.txt
          git commit -m "stage downloading ${{ inputs.job_id }}" || true
          git push origin progress

          sudo apt-get update -y
          sudo apt-get install -y aria2

          mkdir -p work/upload

          python3 << 'EOF'
          import json, pathlib, subprocess, shlex

          with open("work/files.json") as f:
              data = json.load(f)

          for fobj in data["files"]:

              urls = fobj.get("url")

              if not urls:
                  continue

              # support:
              # url = "https://..."
              # or url = ["https://...", "https://mirror..."]
              if isinstance(urls, str):
                  url_list = [urls]
              elif isinstance(urls, list):
                  url_list = urls
              else:
                  continue

              folder = fobj.get("folder","").strip("/")
              name   = fobj.get("final")

              outdir = pathlib.Path("work/upload") / folder
              outdir.mkdir(parents=True, exist_ok=True)

              base_cmd = [
                  "aria2c",
                  "--file-allocation=trunc",
                  "--auto-file-renaming=false",
                  "--summary-interval=0",
                  "-k", "1M",
                  "-o", name,
                  "-d", str(outdir)
              ]

              # fast mode
              fast_cmd = base_cmd + ["-x","8","-s","8"] + url_list

              print("Downloading (fast):", name)
              r = subprocess.run(fast_cmd)

              # fallback to single connection if server blocks ranges
              if r.returncode != 0:
                  print("Retrying single connection:", name)
                  slow_cmd = base_cmd + ["-x","1","-s","1"] + url_list
                  subprocess.run(slow_cmd)

          EOF

      # -----------------------------
      # uploading
      # -----------------------------
      - name: Stage – uploading
        run: |
          echo "uploading" > progress/${{ inputs.job_id }}.txt
          git add progress/${{ inputs.job_id }}.txt
          git commit -m "stage uploading ${{ inputs.job_id }}" || true
          git push origin progress

          mkdir -p ~/.ssh
          echo "${{ secrets.SF_SSHKEY }}" > ~/.ssh/id_ed25519
          chmod 600 ~/.ssh/id_ed25519

          ssh-keyscan frs.sourceforge.net >> ~/.ssh/known_hosts

          rsync -av --partial \
            work/upload/ \
            ${{ secrets.SF_USER }}@frs.sourceforge.net:/home/frs/project/${{ secrets.SF_PROJECT }}/

      # -----------------------------
      # verifying (stage marker)
      # -----------------------------
      - name: Stage – verifying
        run: |
          echo "verifying" > progress/${{ inputs.job_id }}.txt
          git add progress/${{ inputs.job_id }}.txt
          git commit -m "stage verifying ${{ inputs.job_id }}" || true
          git push origin progress

      # --------------------------------------------------
      # result + summary (REMOTE SSH VERIFICATION)
      # --------------------------------------------------
      - name: Build result and summary
        run: |
          mkdir -p progress/result

          python3 << 'EOF'
          import json, time, os, subprocess, shlex

          with open("work/start_time.txt") as f:
              start = int(f.read().strip())

          with open("work/files.json") as f:
              data = json.load(f)

          files = data["files"]
          compression = data.get("compression")

          project = os.environ["SF_PROJECT"]

          total = 0

          def remote_size(path):
              try:
                  cmd = [
                      "ssh",
                      "frs.sourceforge.net",
                      "stat -c %s " + shlex.quote(path)
                  ]
                  out = subprocess.check_output(cmd, stderr=subprocess.DEVNULL, timeout=20, shell=False)
                  return int(out.decode().strip())
              except:
                  return None

          for fobj in files:
              folder = fobj.get("folder","").strip("/")
              name   = fobj.get("final")

              if folder:
                  rpath = f"/home/frs/project/{project}/{folder}/{name}"
              else:
                  rpath = f"/home/frs/project/{project}/{name}"

              size = remote_size(rpath)

              if size is not None:
                  fobj["size"] = size
                  fobj["status"] = "ok"
                  total += size
              else:
                  fobj["size"] = 0
                  if fobj.get("status") != "retry":
                      fobj["status"] = "failed"

          now = int(time.time())

          summary = {
              "total_files": len(files),
              "total_size": total,
              "time_taken": now - start,
              "started_at": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime(start)),
              "finished_at": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime(now)),
              "compression": compression
          }

          out = {
              "files": files,
              "summary": summary,
              "notes": os.environ.get("NOTES","")
          }

          with open("progress/result/%s.json" % os.environ["JOB_ID"],"w") as f:
              json.dump(out,f,indent=2)

          EOF
        env:
          SF_PROJECT: ${{ secrets.SF_PROJECT }}
          NOTES: ${{ inputs.notes }}
          JOB_ID: ${{ inputs.job_id }}

         

         

      # --------------------------------------------------
      # update mirror history
      # --------------------------------------------------
      - name: Update mirror history
        if: always()
        run: |
          mkdir -p progress

          python3 << 'EOF'
          import json, os, time

          path = "progress/history.json"

          job_id = os.environ["JOB_ID"]
          run_id = os.environ["RUN_ID"]
          status = os.environ.get("CONCLUSION","completed")

          now = time.strftime("%Y-%m-%d %H:%M:%S", time.gmtime())

          entry = {
              "job_id": job_id,
              "run_id": int(run_id),
              "status": status,
              "time": now
          }

          try:
              with open(path) as f:
                  arr = json.load(f)
                  if not isinstance(arr, list):
                      arr = []
          except:
              arr = []

          arr.insert(0, entry)
          arr = arr[:10]

          with open(path,"w") as f:
              json.dump(arr,f,indent=2)

          EOF

          git add progress/history.json
          git commit -m "history update ${{ inputs.job_id }}" || true
          git push origin progress
        env:
          JOB_ID: ${{ inputs.job_id }}
          RUN_ID: ${{ github.run_id }}
          CONCLUSION: ${{ job.status }}

      # -----------------------------
      # finished
      # -----------------------------
      - name: Stage – finished
        if: always()
        run: |
          echo "finished" > progress/${{ inputs.job_id }}.txt
          git add progress/${{ inputs.job_id }}.txt
          git commit -m "stage finished ${{ inputs.job_id }}" || true
          git push origin progress
